---
title: "stock_webscraping_PERT_data.Rmd"
author: "Paul N. Leiby"
date: "12/7/2020"
output:
  slidy_presentation:
    footer: "BOIC PERT data"
    theme: cerulean
    highlight: tango
  beamer_presentation:
    colortheme: dolphin
    fonttheme: structurebold
    theme: AnnArbor
  html_document: default
  word_document:
    df_print: kable
    fig_caption: yes
    fig_height: 5
    fig_width: 7
  ioslides_presentation: default

---

- Created by Paul N. Leiby
- Date modified: 2017-07-16

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

hires_dpi = 100 # graph resolution desired at higher level (some docs don't rescale well - set to 100)
echoWorking = F
```

Uses the rvest library.
[rvest: easy web scraping with R (Wickham 2014)](http://blog.rstudio.org/2014/11/24/rvest-easy-web-scraping-with-r/)
[SelectorGadget vignette (H. Wickham)](https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html)
[`rvest` Key functions](https://github.com/tidyverse/rvest)

```{r loadlibraries, echo=echoWorking}
library(tidyverse)
# library(magrittr) # included in tidyversew
library(rvest)
```

This scraping routines applies over lists of stock tickers.

```{r}
dataOutputFileName <- "BOIC_PERTdata_"

BOICtickers <- c(
  "AMZN", "GIII",
  "GOOG", "GPK",
  "HCSG", "JAZZ", "MTZ",
  "SWKS", "THO"
)

BOICtickersHistorical <- c(
  "AAPL", "CBI", "CMI", "EMC",
  "GIII", "GIL",
  "GOOG", "GOOGL",
  "HAR", "HCSG",
  "JAZZ", "LKQ",
  "MSM", "PHM", "QCOM", "R", "WFM",
  "SNCR", "PWR", "SWKS", "UNP"
)


# dataOutputFileName = "StockPicks_"
StockPicks20200712tickers <- c("CAG", "CMCSA", "NFLX", "VIAC")

StockPicks20160410tickers <- c("IRBT", "GOOG", "GOOGL", "DDD", "SSYS") # ARCM
StockPicks20160409tickers <- c(
  "AIV", "AVB", "BXP", "CPT", "CXW", "DDR", "DLR", "DRE",
  "EQR", "ESS", "NLY", "EXR", "FRT", "FCH", "FCE-A", "GEO", "GGP",
  "HCP", "HR",
  "HPT", "HST", "KIM", "LPT", "MAC", "CLI", "PEI", "PLD", "PSA", "O",
  "RHP", "SLG", "SPG", "UDR", "VTR", "VNO", "WPC", "WRE", "WRI", "HCN"
)
StockPicks20160410tickers <- c("IRBT", "GOOG", "GOOGL", "TOSYY", "GDFYF", "DDD", "ARCM")
StockPicks20151018tickers <- c("TRIP", "GOOGL", "BABA", "AMZN", "BIDU", "EBAY", "Z")
# StockPicks20151018tickers = c("BABA","GOOGL")

```

We select one of the lists of stock tickers

```{r}
tickersWanted <- BOICtickers
# tickersWanted = StockPicks20160410tickers
```

```{r}
# URLs and queries for Yahoo Finance pages
# functions to create needed URLs
url_summary <- function(currticker) {
  # paste0("http://finance.yahoo.com/q?s=",currticker,"&ql=1")
  paste0("https://finance.yahoo.com/quote/", currticker, "?p=", currticker)
}

url_analysis <- function(currticker) {
  paste0("https://finance.yahoo.com/quote/", currticker, "/analysis?p=", currticker)
}

# url_analysts <- function(currticker) { # **superseded** - no # longer functions
#   paste0("https://finance.yahoo.com/quote/", currticker, # "/analysts?p=", currticker)
# }

# url_opinion = function(currticker) {
#   paste0("http://finance.yahoo.com/q/ao?s=", currticker, "+Analyst+Opinion")
# }
# url_estimate = function(currticker) {
#   paste0("http://finance.yahoo.com/q/ae?s=", currticker, "+Analyst+Estimates")
# }

as.numeric.pct <- function(pctstring) {
  # Expect a numeric string ending in a percentage symbol
  # Return the string without the percent symbol
  # as.numeric(sub("%","",pctstring))/100
  as.numeric(sub("%", "", pctstring))
}
```

To extract the desired elements, use [selectorgadget](http://selectorgadget.com/) to figure out which css selector matches the data we want: strong span.

- We use
  - `html_node()` to find the first node that matches that selector, 
  - `html_text()` to extract its contents, and 
  - `as.numeric()` convert it to numeric:

- Can extract 
  - the tag names with `html_tag()`, 
  - text with `html_text()`, 
  - a single attribute with `html_attr()` or 
  - all attributes with `html_attrs()`.

- Detect and repair text encoding problems with `guess_encoding()` and `repair_encoding()`.

- Navigate around a website as if youâ€™re in a browser with 
  - `html_session()`, 
  - `jump_to()`, 
  - `follow_link()`, 
  - `back()`, and 
  - `forward()`. 

- Extract, modify and submit forms with 
  - `html_form()`, `set_values()` and `submit_form()`.

```{r defNodesWanted}
# Here we set up the strings that describe html "nodes", where
#  desired info in located on target web pages
#  The content of the node strings is determined manually with the selectorgadget browser add-in.
# <title>AMZN 3,193.23 -10.30 -0.32% : Amazon.com, Inc. - Yahoo Finance</title>
# <span class="Trsdu(0.3s) Fw(b) Fz(36px) Mb(-4px) D(ib)" data-reactid="50">3,203.53</span>
nodeForSummary_CurrentPrice <- function(currticker) {
  # return(paste0("#yfs_l84_", tolower(currticker))) # ??? .Trsdu(0\.3s) ???
  return(".D(ib)")
}

nodeForAnalystsRecommendation <- ".equaltable:nth-child(2) tr:nth-child(1) .yfnc_tabledata1"
nodeForAnalystsRecCountCurrMonth <- "th+ .yfnc_tabledata1"
nodeForAnalystsRecCountLastMonth <- ".yfnc_datamodoutline1 table .yfnc_tabledata1:nth-child(3)"

# Recommendation Rating on Analysis Page
# Look for 'aria-label="N.N on a scale of 1 to 5"'; 'data-test="rec-rating-text"'
# <div class="B(8px) Pos(a) C(white) Py(2px) Px(0) Ta(c) Bdrs(3px) Trstf(eio) Trsde(0.5) Arrow South Bdtc(i)::a Fw(b) Bgc($c-fuji-teal-2-b) Bdtc($c-fuji-teal-2-b)" data-test="rec-rating-txt" tabindex="0" aria-label="1.7 on a scale of 1 to 5, where 1 is Strong Buy and 5 is Sell" style="width: 30px; left: calc(17.5% - 15px);">1.7</div>
nodeForAnalysisRecommendationRating <- ".Bdtc($c-fuji-teal-2-b)" 
# Recommendation counts appear to be hidden in images

# Price on Analysis Page
# <span class="Trsdu(0.3s) Fw(b) Fz(36px) Mb(-4px) D(ib)" data-reactid="50">3,203.53</span>
nodeForAnalysis_Price <- ""

# Growth Rates on Analysis Page
# Following are now in tables, easily scraped to dataframes
nodeFor5YrGrowthEst <-
  ".yfnc_tableout1~ .yfnc_tableout1 tr:nth-child(7) .yfnc_tablehead1+ .yfnc_tabledata1"
nodeFor5YrGrowthEst2 <-".Py(10px)"
nodeForAllGrowthEsts <-
  ".yfnc_tableout1:nth-child(11) .yfnc_tablehead1+ .yfnc_tabledata1"
GrowthEstsNames <- c(
  "GrowthCurrentQtr", "GrowthNextQtr", "GrowthThisYear",
  "GrowthNextYear", "GrowthPast5Years", "GrowthNext5Years",
  "PE_Ratio", "PEG_Ratio"
)
nodeForAllGrowthEstsWithLabels <-
  ".yfnc_tableout1:nth-child(11) td.yfnc_tablehead1 , .yfnc_tableout1:nth-child(11) .yfnc_tablehead1+ .yfnc_tabledata1" # 2-d table
```

Establish function to get data from Yahoo Finance for individual stock ticker
-----------------------------

```{r}
currticker = "AMZN"
```

Read tables in page and explore

https://www.pluralsight.com/guides/advanced-web-scraping-with-r

```{r extract_html_tables}
  # webpage <- read_html(url)
  stockAnalysis <- read_html(url_analysis(currticker))
  allTables <- html_nodes(stockAnalysis, css = "table") # list of matching nodes, "xml_nodeset"
  # oneTable <- html_table(allTables[[1]]) # convert first to dataframe
  #  oneTable
  # class(allTables)
  allTablesDf <- html_table(allTables) # convert to list of all tables, each as a dataframe
  allTablesDf[[1]]
  allTablesDf[[6]]
```

```{r}
allTablesDf[[6]][[currticker]][1] # Growth Estimates; Current Qtr.
names(allTablesDf)
```

```{r}
.D(ib)
nodeForSummary_CurrentPrice

```

```{r}
# Extracting sub URLs
# Here, tile-box is a parent class which holds the content in the nested class.
# First, go inside the sub-class using html_children() and then fetch the URLs to each Skill page
# extract linked html pages
subURLs <- stockAnalysis %>%
  html_nodes('*') %>% # all nodes
  # html_children() %>% 
  html_attr('href')
subURLs <- subURLs[!is.na(subURLs)]
subURLs
```

```{r}

```


```{r}
getOpinionInfo <- function(currticker) {
  
  stockOpinion <- read_html(url_opinion(currticker))

  # get average recommendation from Analysts Opinion page
  nodeWanted <- nodeForAnalystsRecommendation
  aveRecommendation <- stockOpinion %>%
    html_node(nodeWanted) %>%
    html_text() # %>% as.numeric()

  # get count of each recommendation from Analysts Opinion page - Curr Month
  nodeWanted <- nodeForAnalystsRecCountCurrMonth
  allRecommendationsCurrM <- stockOpinion %>%
    html_nodes(nodeWanted) %>%
    html_text() # %>% as.numeric()

  # get count of each recommendation from Analysts Opinion page - Last Month
  nodeWanted <- nodeForAnalystsRecCountLastMonth
  allRecommendationsLastM <- stockOpinion %>%
    html_nodes(nodeWanted) %>%
    html_text() # %>% as.numeric()
}
```

```{r defFn_getDataForOneTicker}
# html_node() get first match, html_nodes() gets all matches
# html_text() extracts text from node

getDataForOneTicker <- function(currticker) {
  # get a range of info on stock from Yahoo Finance pages for given ticker,
  # return as an (unlabeled) vector

  # read in pages (as html), only once and store
  stockSummary <- read_html(url_summary(currticker))
  
  stockAnalysis <- read_html(url_analysis(currticker))

  # get 5-year growth estimate from Analysis page
  nodeWanted <- nodeFor5YrGrowthEst2
  growthEstimate <- stockAnalysis %>%
    html_node(nodeWanted) %>%
    html_text() # %>% as.numeric.pct()

  # get vector of past growth rates and future growth estimates from Analysis page
  nodeWanted <- nodeForAllGrowthEsts
  allGrowthEstimates <- stockEstimate %>%
    html_nodes(nodeWanted) %>%
    html_text() # %>% as.numeric.pct()

  # get 2-d table of past growth rates and future growth est from Analysts Estimate page
  nodeWanted <- nodeForAllGrowthEstsWithLabels
  allGrowthEstimatesWithLabels <- stockEstimate %>%
    html_nodes(nodeWanted) %>%
    html_text()
  allGrowthEstimatesWithLabels <- matrix(allGrowthEstimatesWithLabels, ncol = 2, byrow = T)
  # allGrowthEstimatesWithLabels[,2] = as.numeric.pct(allGrowthEstimatesWithLabels[,2])

  # get current price from Summary Page
  nodeWanted <- nodeForSummary_CurrentPrice(currticker)
  currentPrice <- stockSummary %>%
    html_node(nodeWanted) %>%
    html_text() # %>% as.numeric()

  allRecCurrM <- paste0(allRecommendationsCurrM, collapse = ",")
  allRecLastM <- paste0(allRecommendationsLastM, collapse = ",")

  return(c(currentPrice, growthEstimate, aveRecommendation, allRecCurrM, allRecLastM, allRecommendationsCurrM, allRecommendationsLastM))
}
```

Now loop over stocks in the portfolio list, developing data frame of info
-------------

```{r loopForStockInfo}
tickersWanted <- StockPicks20200712tickers

# stockInfo = data.frame(matrix(ncol = 15, nrow = length(tickersWanted)))
stockInfo <- data.frame(setNames(
  replicate(15, character(0), simplify = F),
  c(
    "Price", "EPSgrowth", "AveRec", "RecCurrM", "RecLastM",
    paste0(rep("Rec1CurrM", 5), 1:5), paste0(rep("Rec1LastM", 5), 1:5)
  )
))
stockInfoL <- list()

for (ticker in tickersWanted) {
  # stockInfo = rbind(stockInfo,getDataForOneTicker(ticker))
  stockInfoL[ticker] <- list(getDataForOneTicker(ticker))
  print(stockInfoL[ticker])
}

stockInfoL["vars"] <- list(c(
  "Price", "EPSgrowth", "AveRec", "RecCurrM", "RecLastM",
  paste0(rep("Rec1CurrM", 5), 1:5), paste0(rep("Rec1LastM", 5), 1:5)
))
# rownames(stockInfo) = tickersWanted

# display the result table
stockInfoL

# save to a csv file, with date-stamp in name
write.csv(stockInfoL, file = paste0(dataOutputFileName, format(Sys.time(), "%Y-%m-%d"), ".csv"))
```

Extract Historical Price Data if Wanted

``` {r}
# Download URLs for historical price data,
# Daily, Mar 27, 2014 to Oct 10, 2015
# http://real-chart.finance.yahoo.com/table.csv?s=GOOG&d=9&e=10&f=2015&g=d&a=2&b=27&c=2014&ignore=.csv
# Daily, Mar 27, 2014 to Oct 9, 2015
# http://real-chart.finance.yahoo.com/table.csv?s=GOOG&a=00&b=1&c=1960&d=08&e=9&f=2015&g=d&ignore=.csv
# a = startMonthnum-1 # 2 digits, 00 = Jan
# b = startDay        # 1 or 2 digits, 1 = 1
# c = startYear       # digits
# d = startMonthnum-1 # 2 digits, 00 = Jan
# e = startDay        # 1 or 2 digits, 1 = 1
# f = endYear
# s = ticker
# g = period          # d for daily, w = weekly, m = monthly, v = dividends_only
# http://finance.yahoo.com/q/hp?s=GOOG&a=00&b=1&c=1960&d=08&e=9&f=2015&g=d

# format(Sys.time(), "%a %b %d %X %Y")
# format(Sys.time(), "%a %b %d %H:%M:%S %Y")

getPriceHistory <- function(currticker = "", startDate = "1960-01-01", endDate = "") {
  # expect dates as yyyy-mm-dd
  # default to end with current date
  if (endDate == "") endDate <- format(Sys.time(), "%Y-%m-%d")
  ql <- list() # query lisrt
  # y,m,d = 1,2,3
  ql[1:3] <- strsplit(startDate, split = "-")[[1]] # yr, month, day
  ql[4:6] <- strsplit(endDate, split = "-")[[1]]
  priceHistURL <- paste0(
    "http://real-chart.finance.yahoo.com/table.csv?s=", currticker,
    "&a=", "00", "&b=", ql[3], "&c=", ql[1],
    "&d=", "08", "&e=", ql[6], "&f=", ql[4],
    "&g=d&ignore=.csv"
  )
  return(read.csv(url(priceHistURL)))
}

# priceData = getPriceHistory("AAPL")
```

Read SSG Data File and Parse
-----------

### INCOMPLETE!!

```{r readSSG}
SSGfields <- read.csv(file = "SSGDBfields.csv", header = F)
SSG_AAPL <- readLines("AAPL_20151018.ssg")
```

